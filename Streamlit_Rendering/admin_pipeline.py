# Streamlit_Rendering/admin_pipeline.py

import json
import pandas as pd
import numpy as np
import streamlit as st

from Streamlit_Rendering.crawl import fetch_article_from_url
from Streamlit_Rendering import repo
# summary.pyì—ì„œ í´ëž˜ìŠ¤ì™€ ë”ë¯¸ í•¨ìˆ˜ ê°€ì ¸ì˜¤ê¸°
from Streamlit_Rendering.summary import FastKoBertSummarizer, summarize_text_dummy
from Streamlit_Rendering.trust import score_trust_dummy

# --------------------------------------------------------------------------
# 1. ëª¨ë¸ ìºì‹± (ê°€ìž¥ ì¤‘ìš”!)
# Streamlitì€ ìƒˆë¡œê³ ì¹¨í•  ë•Œë§ˆë‹¤ ì½”ë“œë¥¼ ë‹¤ì‹œ ì‹¤í–‰í•˜ëŠ”ë°, 
# ëª¨ë¸ ë¡œë”©ì„ ë§¤ë²ˆ í•˜ë©´ ì„œë²„ê°€ í„°ì§‘ë‹ˆë‹¤. ì´ë¥¼ ë°©ì§€í•˜ëŠ” ì½”ë“œìž…ë‹ˆë‹¤.
# --------------------------------------------------------------------------
@st.cache_resource
def load_summarizer_model():
    """
    FastKoBertSummarizer ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— í•œ ë²ˆë§Œ ì˜¬ë¦¬ê³  ìž¬ì‚¬ìš©í•©ë‹ˆë‹¤.
    """
    print("ðŸš€ Loading FastKoBertSummarizer... (First time only)")
    model = FastKoBertSummarizer()
    return model

# --------------------------------------------------------------------------
# 2. ë©”ì¸ ë¡œì§
# --------------------------------------------------------------------------

ARTICLE_COLUMNS = [
    "article_id", "title", "source", "url", "published_at", "full_text",
    "summary_text", "keywords", "embed_full", "embed_summary",
    "trust_score", "trust_verdict", "trust_reason", "trust_per_criteria",
    "status",
]

def ingest_one_url(url: str, source: str = "manual", dedup_by_url: bool = True) -> dict:
    """
    URL 1ê°œ â†’ í¬ë¡¤ë§ â†’ (ì¤‘ë³µ í•„í„°ë§) â†’ ëª¨ë¸ ë¶„ì„ â†’ DB ì ìž¬
    """
    try:
        # 1. ì¤‘ë³µ ì²´í¬
        if dedup_by_url and repo.exists_article_url(url):
            return {"status": "skipped", "message": "ì´ë¯¸ DBì— ì¡´ìž¬í•˜ëŠ” URLìž…ë‹ˆë‹¤. (ì¤‘ë³µ ìŠ¤í‚µ)", "url": url}

        # 2. í¬ë¡¤ë§
        df_raw = fetch_article_from_url(url=url, source=source)
        
        # 3. ë°ì´í„° ê°€ê³µ ë° ëª¨ë¸ ì‹¤í–‰ (ì—¬ê¸°ê°€ í•µì‹¬)
        df_ready = build_ready_rows(df_raw)

        # 4. DB ì ìž¬
        repo.upsert_articles(df_ready)
        return {"status": "inserted", "message": "DBì— 1ê±´ ì ìž¬ë˜ì—ˆìŠµë‹ˆë‹¤.", "url": url}

    except Exception as e:
        return {"status": "error", "message": f"í¬ë¡¤ë§/ì ìž¬ ì‹¤íŒ¨: {e}", "url": url}


def build_ready_rows(df_raw: pd.DataFrame) -> pd.DataFrame:
    """
    í¬ë¡¤ë§ëœ ë°ì´í„°ë¥¼ ë°›ì•„ ëª¨ë¸(FastKoBertSummarizer)ì„ ëŒë ¤ 
    ìš”ì•½, í‚¤ì›Œë“œ, ìž„ë² ë”©ì„ ì±„ì›Œ ë„£ìŠµë‹ˆë‹¤.
    """
    # ìºì‹±ëœ ëª¨ë¸ ë¶ˆëŸ¬ì˜¤ê¸°
    model = load_summarizer_model()
    
    rows = []
    for _, r in df_raw.iterrows():
        full_text = str(r["full_text"])
        source = str(r["source"])
        
        # -------------------------------------------------------
        # [í•µì‹¬] ëª¨ë¸ analyze_single ë©”ì„œë“œ í•œ ë²ˆ í˜¸ì¶œë¡œ ëª¨ë“  ê°’ íšë“
        # ë°˜í™˜ê°’ ìˆœì„œ: summary, keywords, content_emb, keyword_emb, summary_emb, trust_score
        # -------------------------------------------------------
        try:
            summary, keywords, content_emb, keyword_emb, summary_emb, trust_score_model = model.analyze_single(full_text)
            
            # Numpy ë°°ì—´ì„ JSON ì €ìž¥ì„ ìœ„í•´ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            embed_full_list = content_emb.tolist() if hasattr(content_emb, 'tolist') else []
            embed_summary_list = summary_emb.tolist() if hasattr(summary_emb, 'tolist') else []
            
        except Exception as e:
            print(f"âŒ Model Analysis Error: {e}")
            # ì—ëŸ¬ ë°œìƒ ì‹œ ë”ë¯¸ ê°’ìœ¼ë¡œ ëŒ€ì²´
            summary = summarize_text_dummy(full_text)
            keywords = []
            embed_full_list = []
            embed_summary_list = []
            trust_score_model = 50

        # ì‹ ë¢°ë„ ìƒì„¸ í‰ê°€ (Trust ë¡œì§ì€ ë³„ë„ í•¨ìˆ˜ì™€ ë³‘í–‰ ì‚¬ìš©)
        trust_detail = score_trust_dummy(full_text, source=source, low=30, high=100)
        
        # ëª¨ë¸ ì ìˆ˜ì™€ ë£°ë² ì´ìŠ¤ ì ìˆ˜ ì¤‘ ëª¨ë¸ ì ìˆ˜ë¥¼ ìš°ì„ í•˜ê±°ë‚˜ í‰ê· ì„ ë‚¼ ìˆ˜ ìžˆìŒ
        # ì—¬ê¸°ì„œëŠ” ëª¨ë¸ ì ìˆ˜ë¥¼ ìš°ì„ ìœ¼ë¡œ ë„£ìŒ
        final_trust_score = int(trust_score_model)

        rows.append({
            "article_id": str(r["article_id"]),
            "title": str(r["title"]),
            "source": source,
            "url": str(r["url"]),
            "published_at": str(r["published_at"]),
            "full_text": full_text,

            # ëª¨ë¸ ë¶„ì„ ê²°ê³¼ ë§¤í•‘
            "summary_text": summary,
            "keywords": json.dumps(keywords, ensure_ascii=False), # ë¦¬ìŠ¤íŠ¸ -> JSON ë¬¸ìžì—´
            "embed_full": json.dumps(embed_full_list),            # ë¦¬ìŠ¤íŠ¸ -> JSON ë¬¸ìžì—´
            "embed_summary": json.dumps(embed_summary_list),      # ë¦¬ìŠ¤íŠ¸ -> JSON ë¬¸ìžì—´

            # ì‹ ë¢°ë„ ì •ë³´
            "trust_score": final_trust_score,
            "trust_verdict": trust_detail.get("verdict", "uncertain"),
            "trust_reason": trust_detail.get("reason", ""),
            "trust_per_criteria": json.dumps(trust_detail.get("per_criteria", {}), ensure_ascii=False),

            "status": "ready",
        })

    df_ready = pd.DataFrame(rows).reindex(columns=ARTICLE_COLUMNS)
    return df_ready

# --------------------------------------------------------------------------
# ê°œë³„ í…ŒìŠ¤íŠ¸ìš© í•¨ìˆ˜ (í•„ìš”í•œ ê²½ìš°ì—ë§Œ ì‚¬ìš©)
# build_ready_rowsì—ì„œ ì´ë¯¸ ë‹¤ ì²˜ë¦¬í•˜ë¯€ë¡œ ì‹¤ì œ íŒŒì´í”„ë¼ì¸ì—ì„œëŠ” ìž˜ ì•ˆ ì“°ìž„
# --------------------------------------------------------------------------

def run_summary(full_text: str) -> str:
    model = load_summarizer_model()
    summary, _, _, _, _, _ = model.analyze_single(full_text)
    return summary

def run_keywords(full_text: str) -> list[str]:
    model = load_summarizer_model()
    _, keywords, _, _, _, _ = model.analyze_single(full_text)
    return keywords

def run_embedding(text: str) -> list[float]:
    model = load_summarizer_model()
    # ìž„ë² ë”©ë§Œ í•„ìš”í•  ë•Œ (Batch ì²˜ë¦¬ í™œìš©)
    emb = model.get_embedding_batch([text])[0]
    return emb.tolist()

def run_trust(full_text: str, source: str
